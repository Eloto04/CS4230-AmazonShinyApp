{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90046f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for network analysis\n",
    "import networkx as nx\n",
    "import random\n",
    "from infomap import Infomap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import directed graph from .txt edge list\n",
    "G = nx.read_edgelist('AmazonGraph.txt', \n",
    "                     create_using=nx.DiGraph(), \n",
    "                     nodetype=int,\n",
    "                     data=False)\n",
    "\n",
    "print(f\"Graph loaded successfully!\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31900f24",
   "metadata": {},
   "source": [
    "### Centrality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node\n",
    "# alpha=0.85 is the standard damping parameter\n",
    "print(\"\\nComputing PageRank...\")\n",
    "pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "\n",
    "# Show the top 10 most important nodes\n",
    "print(\"\\nTop 10 nodes by PageRank:\")\n",
    "top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top_pagerank:\n",
    "    print(f\"  Node {node}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6091e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Eigenvector Centrality\n",
    "print(\"Computing Eigenvector Centrality...\")\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-6)\n",
    "\n",
    "# Show the top 10 nodes\n",
    "print(\"\\nTop 10 nodes by Eigenvector Centrality:\")\n",
    "top_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top_eigenvector:\n",
    "    print(f\"  Node {node}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Degree Centrality\n",
    "print(\"Computing Degree Centrality...\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Show the top 10 nodes\n",
    "print(\"\\nTop 10 nodes by Degree Centrality:\")\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top_degree:\n",
    "    print(f\"  Node {node}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14318fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Betweenness Centrality\n",
    "print(\"Computing Betweenness Centrality...\")\n",
    "betweenness_centrality = nx.betweenness_centrality(G, k=1000)  # Sample 1000 nodes for speed\n",
    "\n",
    "# Show the top 10 nodes\n",
    "print(\"\\nTop 10 nodes by Betweenness Centrality:\")\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top_betweenness:\n",
    "    print(f\"  Node {node}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a4a2e",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Infomap to find communities in the network\n",
    "print(\"Running Infomap community detection...\")\n",
    "im = Infomap(directed=True, silent=True)\n",
    "\n",
    "# Add all the edges to Infomap\n",
    "for u, v in G.edges():\n",
    "    im.add_link(u, v)\n",
    "\n",
    "# Run the algorithm\n",
    "im.run()\n",
    "\n",
    "# Get the communities from the results\n",
    "communities = {}\n",
    "node_to_community = {}\n",
    "\n",
    "for node in im.nodes:\n",
    "    node_id = node.node_id\n",
    "    module_id = node.module_id\n",
    "    # Add node to its community\n",
    "    if module_id not in communities:\n",
    "        communities[module_id] = []\n",
    "    communities[module_id].append(node_id)\n",
    "    node_to_community[node_id] = module_id\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"\\nTotal communities found: {len(communities)}\")\n",
    "print(f\"Codelength (compression): {im.codelength:.4f}\")\n",
    "\n",
    "# Look at community sizes\n",
    "community_sizes = sorted([len(nodes) for nodes in communities.values()], reverse=True)\n",
    "print(f\"\\nLargest communities (top 10):\")\n",
    "for i, size in enumerate(community_sizes[:10], 1):\n",
    "    print(f\"  Community {i}: {size} nodes\")\n",
    "\n",
    "print(f\"\\nSmallest community: {community_sizes[-1]} nodes\")\n",
    "print(f\"Average community size: {sum(community_sizes)/len(community_sizes):.2f} nodes\")\n",
    "\n",
    "# Save community info to the graph\n",
    "nx.set_node_attributes(G, node_to_community, 'community')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862daa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the communities and calculate modularity\n",
    "\n",
    "# Calculate modularity Q value\n",
    "community_sets = [set(nodes) for nodes in communities.values()]\n",
    "modularity_q = nx.algorithms.community.modularity(G, community_sets)\n",
    "\n",
    "print(f\"Network Community Analysis:\")\n",
    "print(f\"Number of detected communities: {len(communities)}\")\n",
    "print(f\"Modularity Q: {modularity_q:.4f}\")\n",
    "print(f\"(Q ranges from -0.5 to 1.0; higher is better)\\n\")\n",
    "\n",
    "# Sample some nodes to visualize (can't plot all of them)\n",
    "# Just take nodes from the biggest communities\n",
    "n_nodes_to_sample = 500\n",
    "largest_community_ids = sorted(communities.keys(), \n",
    "                               key=lambda x: len(communities[x]), \n",
    "                               reverse=True)[:5]\n",
    "nodes_to_plot = []\n",
    "for comm_id in largest_community_ids:\n",
    "    nodes_to_plot.extend(communities[comm_id][:n_nodes_to_sample // 5])\n",
    "\n",
    "# Create subgraph\n",
    "G_sample = G.subgraph(nodes_to_plot).copy()\n",
    "\n",
    "# Assign colors based on community\n",
    "node_colors = [node_to_community[node] for node in G_sample.nodes()]\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G_sample, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Draw the network\n",
    "nx.draw_networkx_nodes(G_sample, pos, \n",
    "                       node_color=node_colors,\n",
    "                       node_size=50,\n",
    "                       cmap='tab20',\n",
    "                       alpha=0.8)\n",
    "\n",
    "nx.draw_networkx_edges(G_sample, pos, \n",
    "                       alpha=0.2,\n",
    "                       width=0.5,\n",
    "                       arrows=True,\n",
    "                       arrowsize=5,\n",
    "                       arrowstyle='->')\n",
    "\n",
    "plt.title(f'Network Visualization Colored by Community\\n({len(G_sample.nodes())} nodes from top 5 communities)\\nModularity Q = {modularity_q:.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualized {len(G_sample.nodes())} nodes and {len(G_sample.edges())} edges from the largest communities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eafe33",
   "metadata": {},
   "source": [
    "### Diffusion or Threshold Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9395c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick seed nodes for the diffusion models\n",
    "# Using out-degree first, then in-degree to break ties since many nodes have max out-degree of 5\n",
    "node_scores = []\n",
    "for node in G.nodes():\n",
    "    out_deg = G.out_degree(node)\n",
    "    in_deg = G.in_degree(node)\n",
    "    combined_score = (out_deg, in_deg)\n",
    "    node_scores.append((node, out_deg, in_deg, combined_score))\n",
    "\n",
    "# Sort and get top 10\n",
    "top_seeds = sorted(node_scores, key=lambda x: x[3], reverse=True)[:10]\n",
    "seed_nodes = [node for node, out_deg, in_deg, score in top_seeds]\n",
    "\n",
    "print(\"Selected top 10 seed nodes:\")\n",
    "print(\"\\n  Node | Out-Degree | In-Degree | Total\")\n",
    "print(\"-\" * 40)\n",
    "for node, out_deg, in_deg, score in top_seeds:\n",
    "    print(f\"{node:6d} | {out_deg:10d} | {in_deg:9d} | {out_deg + in_deg:5d}\")\n",
    "    \n",
    "print(f\"\\nSeed nodes: {seed_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42d459",
   "metadata": {},
   "source": [
    "#### SI Model\n",
    "\n",
    "\"Infection\" represents a product being purchased. Tracking \"infected\" products = tracking customer interest. Each infected product can \"infect\" other products (customers buy linked products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_model(G, seed_nodes, beta=0.1, max_steps=20):\n",
    "    infected = set(seed_nodes)\n",
    "    infected_over_time = [len(infected)]\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        newly_infected = set()\n",
    "        \n",
    "        for node in infected:\n",
    "            neighbors = list(G.successors(node))\n",
    "            \n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in infected:\n",
    "                    if random.random() < beta:\n",
    "                        newly_infected.add(neighbor)\n",
    "\n",
    "        # Updated infected count after each timestep\n",
    "        infected.update(newly_infected)\n",
    "        infected_over_time.append(len(infected))\n",
    "        \n",
    "        # Stop if nothing new got infected\n",
    "        if len(newly_infected) == 0:\n",
    "            print(f\"SI model converged at step {step + 1}\")\n",
    "            break\n",
    "    \n",
    "    return infected_over_time, infected\n",
    "\n",
    "beta = 0.1\n",
    "\n",
    "# Run SI model\n",
    "print(\"Running SI Model...\")\n",
    "print(f\"Initial seed nodes: {seed_nodes}\")\n",
    "print(f\"Infection probability (beta): {beta}\\n\")\n",
    "\n",
    "cumulative_infected, final_infected = si_model(G, seed_nodes, beta=beta, max_steps=20)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Total infected nodes: {len(final_infected)}\")\n",
    "print(f\"Percentage of network infected: {100 * len(final_infected) / G.number_of_nodes():.2f}%\")\n",
    "print(f\"Total timesteps: {len(cumulative_infected) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d446d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot infected nodes over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(len(cumulative_infected)), cumulative_infected, \n",
    "         'r-', linewidth=2, marker='o', markersize=5, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Timestep', fontsize=12)\n",
    "plt.ylabel('Number of Infected Nodes', fontsize=12)\n",
    "plt.title('SI Model for Amazon co-purchasing graph', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4787bb",
   "metadata": {},
   "source": [
    "#### SIR Model\n",
    "\n",
    "Adds \"recovery\", so products \"recover\" after some time. This represents declining customer interest or seasonal products losing popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_model(G, seed_nodes, beta=0.1, gamma=0.05, max_steps=20):\n",
    "    susceptible = set(G.nodes()) - set(seed_nodes)\n",
    "    infected = set(seed_nodes)\n",
    "    recovered = set()\n",
    "    \n",
    "    s_over_time = [len(susceptible)]\n",
    "    i_over_time = [len(infected)]\n",
    "    r_over_time = [len(recovered)]\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        newly_infected = set()\n",
    "        newly_recovered = set()\n",
    "        \n",
    "        # Each node may infect its neighbors\n",
    "        for node in infected:\n",
    "            neighbors = list(G.successors(node))\n",
    "            \n",
    "            for neighbor in neighbors:\n",
    "                if neighbor in susceptible:\n",
    "                    if random.random() < beta:\n",
    "                        newly_infected.add(neighbor)\n",
    "        \n",
    "        # Recovery probability\n",
    "        for node in infected:\n",
    "            if random.random() < gamma:\n",
    "                newly_recovered.add(node)\n",
    "        \n",
    "        # Update all states\n",
    "        susceptible -= newly_infected\n",
    "        infected -= newly_recovered\n",
    "        infected |= newly_infected\n",
    "        recovered |= newly_recovered\n",
    "        \n",
    "        s_over_time.append(len(susceptible))\n",
    "        i_over_time.append(len(infected))\n",
    "        r_over_time.append(len(recovered))\n",
    "        \n",
    "        # Stop if no one is infected anymore\n",
    "        if len(infected) == 0:\n",
    "            print(f\"SIR model converged at step {step + 1} (no infected nodes remain)\")\n",
    "            break\n",
    "    \n",
    "    return s_over_time, i_over_time, r_over_time\n",
    "\n",
    "beta = 0.2\n",
    "gamma = 0.05\n",
    "\n",
    "# Run SIR model\n",
    "print(\"Running SIR Model...\")\n",
    "print(f\"Initial seed nodes: {seed_nodes}\")\n",
    "print(f\"Infection probability (beta): {beta}\")\n",
    "print(f\"Recovery probability (gamma): {gamma}\\n\")\n",
    "\n",
    "s_counts, i_counts, r_counts = sir_model(G, seed_nodes, beta=beta, gamma=gamma, max_steps=20)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Susceptible nodes: {s_counts[-1]}\")\n",
    "print(f\"Infected nodes: {i_counts[-1]}\")\n",
    "print(f\"Recovered nodes: {r_counts[-1]}\")\n",
    "print(f\"Total timesteps: {len(i_counts) - 1}\")\n",
    "print(f\"Percentage recovered: {100 * r_counts[-1] / G.number_of_nodes():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e893d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot I and R over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "timesteps = range(len(i_counts))\n",
    "\n",
    "plt.plot(timesteps, i_counts, 'r-', linewidth=2, marker='s', \n",
    "         markersize=4, label='Infected (I)', alpha=0.7)\n",
    "plt.plot(timesteps, r_counts, 'g-', linewidth=2, marker='^', \n",
    "         markersize=4, label='Recovered (R)', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Timestep', fontsize=12)\n",
    "plt.ylabel('Number of Nodes', fontsize=12)\n",
    "plt.title('SIR Model for Amazon co-purchasing graph', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate peak infection\n",
    "peak_infected = max(i_counts)\n",
    "peak_time = i_counts.index(peak_infected)\n",
    "print(f\"\\nPeak infection: {peak_infected} nodes at timestep {peak_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce93f07",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Comparing the real Amazon network with Erdos-Renyi, Watts-Strogatz, and Barabasi-Albert simulated graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f50c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller sample for comparison\n",
    "n_sample = 5000  # Sample size for comparison\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "avg_degree = 2 * m / n\n",
    "\n",
    "print(f\"Amazon Network Parameters:\")\n",
    "print(f\"Nodes (n): {n}\")\n",
    "print(f\"Edges (m): {m}\")\n",
    "print(f\"Average degree: {avg_degree:.2f}\")\n",
    "print()\n",
    "\n",
    "# Sample the real network using BFS to preserve connectivity\n",
    "print(f\"Sampling {n_sample} nodes using BFS (preserves local structure)...\\n\")\n",
    "random.seed(42)\n",
    "\n",
    "# Start from a random seed node and do BFS\n",
    "start_node = random.choice(list(G.nodes()))\n",
    "sampled_nodes = set()\n",
    "queue = [start_node]\n",
    "visited = {start_node}\n",
    "\n",
    "while len(sampled_nodes) < n_sample and queue:\n",
    "    node = queue.pop(0)\n",
    "    sampled_nodes.add(node)\n",
    "    \n",
    "    # Get neighbors (both successors and predecessors for better connectivity)\n",
    "    neighbors = list(set(G.successors(node)) | set(G.predecessors(node)))\n",
    "    random.shuffle(neighbors)  # Randomize order to avoid bias\n",
    "    \n",
    "    for neighbor in neighbors:\n",
    "        if neighbor not in visited and len(sampled_nodes) + len(queue) < n_sample:\n",
    "            visited.add(neighbor)\n",
    "            queue.append(neighbor)\n",
    "\n",
    "# Create subgraph from BFS sample\n",
    "G_sample = G.subgraph(list(sampled_nodes)).copy()\n",
    "print(f\"Amazon (BFS sampled): {G_sample.number_of_nodes()} nodes, {G_sample.number_of_edges()} edges\")\n",
    "\n",
    "# Generate synthetic networks with same size as sample\n",
    "print(f\"\\nGenerating synthetic networks with n={n_sample} nodes...\\n\")\n",
    "\n",
    "# 1. Erdos-Renyi\n",
    "p_er = avg_degree / (n_sample - 1)\n",
    "G_er = nx.erdos_renyi_graph(n_sample, p_er, directed=True)\n",
    "print(f\"Erdos-Renyi: {G_er.number_of_nodes()} nodes, {G_er.number_of_edges()} edges\")\n",
    "\n",
    "# 2. Watts-Strogatz\n",
    "k_ws = max(4, int(avg_degree))\n",
    "if k_ws % 2 == 1:  # k needs to be even\n",
    "    k_ws += 1\n",
    "p_ws = 0.1\n",
    "G_ws = nx.watts_strogatz_graph(n_sample, k_ws, p_ws)\n",
    "G_ws = G_ws.to_directed()\n",
    "print(f\"Watts-Strogatz: {G_ws.number_of_nodes()} nodes, {G_ws.number_of_edges()} edges\")\n",
    "\n",
    "# 3. Barabasi-Albert\n",
    "m_ba = max(1, int(avg_degree / 2))\n",
    "G_ba = nx.barabasi_albert_graph(n_sample, m_ba)\n",
    "G_ba = G_ba.to_directed()\n",
    "print(f\"Barabasi-Albert: {G_ba.number_of_nodes()} nodes, {G_ba.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare network properties\n",
    "\n",
    "def analyze_network(G, name):\n",
    "    # Convert to undirected for certain calculations\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Degree statistics\n",
    "    degrees = dict(G.degree())\n",
    "    degree_vals = list(degrees.values())\n",
    "    avg_degree = np.mean(degree_vals)\n",
    "    max_degree = max(degree_vals)\n",
    "    \n",
    "    # Average clustering coefficient\n",
    "    avg_clustering = nx.average_clustering(G_undirected)\n",
    "    \n",
    "    # Largest connected component\n",
    "    components = list(nx.connected_components(G_undirected))\n",
    "    gcc = G_undirected.subgraph(max(components, key=len))\n",
    "    \n",
    "    # Average path length\n",
    "    avg_path_length = nx.average_shortest_path_length(gcc)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Average Clustering Coefficient: {avg_clustering:.4f}\")\n",
    "    print(f\"  Average Path Length (LCC): {avg_path_length:.4f}\")\n",
    "    print(f\"  Average Degree: {avg_degree:.2f}\")\n",
    "    print(f\"  Max Degree: {max_degree}\")\n",
    "    print(f\"  Nodes in largest component: {gcc.number_of_nodes()} ({100*gcc.number_of_nodes()/G.number_of_nodes():.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'clustering': avg_clustering,\n",
    "        'path_length': avg_path_length,\n",
    "        'avg_degree': avg_degree,\n",
    "        'max_degree': max_degree,\n",
    "        'degrees': degree_vals\n",
    "    }\n",
    "\n",
    "# Analyze all four networks (using sampled Amazon graph)\n",
    "print(\"\\nAnalyzing network properties...\")\n",
    "results = []\n",
    "results.append(analyze_network(G_sample, \"Amazon (BFS sampled)\"))\n",
    "results.append(analyze_network(G_er, \"Erdos-Renyi (Random)\"))\n",
    "results.append(analyze_network(G_ws, \"Watts-Strogatz (Small-World)\"))\n",
    "results.append(analyze_network(G_ba, \"Barabasi-Albert (Scale-Free)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d750eb",
   "metadata": {},
   "source": [
    "### Creative or Extended Analysis\n",
    "\n",
    "Link product metadata (categories, reviews, etc.) to graph nodes for enriched analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc3dddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 products from metadata\n",
      "\n",
      "Top 20 Most Popular Products (by PageRank)\n",
      "\n",
      "1. Node 548091 (PageRank: 0.002062)\n",
      "   Title: Laura\n",
      "   Group: DVD\n",
      "   Sales Rank: 110\n",
      "   Reviews: 83 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 549 | Out-Degree: 0\n",
      "\n",
      "2. Node 502784 (PageRank: 0.001446)\n",
      "   Title: Brown Bear, Brown Bear, What Do You See?\n",
      "   Group: Book\n",
      "   Sales Rank: 171\n",
      "   Reviews: 172 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 216 | Out-Degree: 5\n",
      "\n",
      "3. Node 296016 (PageRank: 0.001369)\n",
      "   Title: Easy Spanish Phrase Book: Over 770 Basic Phrases for Everyday Use\n",
      "   Group: Book\n",
      "   Sales Rank: 122\n",
      "   Reviews: 13 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 212 | Out-Degree: 5\n",
      "\n",
      "4. Node 515301 (PageRank: 0.001338)\n",
      "   Title: 1001 Most Useful Spanish Words (Beginners' Guides)\n",
      "   Group: Book\n",
      "   Sales Rank: 290\n",
      "   Reviews: 35 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 228 | Out-Degree: 5\n",
      "\n",
      "5. Node 265965 (PageRank: 0.001245)\n",
      "   Title: Chronicles, Vol. 1\n",
      "   Group: Book\n",
      "   Sales Rank: 651\n",
      "   Reviews: 176 | Avg Rating: 4.0/5.0\n",
      "   In-Degree: 118 | Out-Degree: 1\n",
      "\n",
      "6. Node 502086 (PageRank: 0.001245)\n",
      "   Title: Goodnight Moon (Board Book)\n",
      "   Group: Book\n",
      "   Sales Rank: 156\n",
      "   Reviews: 339 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 124 | Out-Degree: 5\n",
      "\n",
      "7. Node 425735 (PageRank: 0.001225)\n",
      "   Title: Middlesex: A Novel\n",
      "   Group: Book\n",
      "   Sales Rank: 133\n",
      "   Reviews: 517 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 94 | Out-Degree: 1\n",
      "\n",
      "8. Node 335281 (PageRank: 0.001218)\n",
      "   Title: Where Is Baby's Belly Button?\n",
      "   Group: Book\n",
      "   Sales Rank: 250\n",
      "   Reviews: 90 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 86 | Out-Degree: 5\n",
      "\n",
      "9. Node 180888 (PageRank: 0.001178)\n",
      "   Title: The Very Hungry Caterpillar board book\n",
      "   Group: Book\n",
      "   Sales Rank: 279\n",
      "   Reviews: 164 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 116 | Out-Degree: 5\n",
      "\n",
      "10. Node 186356 (PageRank: 0.001136)\n",
      "   Title: 501 Spanish Verbs: Fully Conjugated in All the Tenses in A New Easy-To-Learn Format Alphabetically Arranged\n",
      "   Group: Book\n",
      "   Sales Rank: 398\n",
      "   Reviews: 116 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 142 | Out-Degree: 5\n",
      "\n",
      "11. Node 231267 (PageRank: 0.001112)\n",
      "   Title: The Ultimate French Review and Practice: Mastering French Grammar for Confident Communication\n",
      "   Group: Book\n",
      "   Sales Rank: 2578\n",
      "   Reviews: 2 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 119 | Out-Degree: 1\n",
      "\n",
      "12. Node 484602 (PageRank: 0.001006)\n",
      "   Title: Kind of Blue\n",
      "   Group: Music\n",
      "   Sales Rank: 117\n",
      "   Reviews: 541 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 92 | Out-Degree: 4\n",
      "\n",
      "13. Node 243404 (PageRank: 0.000979)\n",
      "   Title: Oh, Inverted World\n",
      "   Group: Music\n",
      "   Sales Rank: 94\n",
      "   Reviews: 233 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 38 | Out-Degree: 2\n",
      "\n",
      "14. Node 239107 (PageRank: 0.000926)\n",
      "   Title: The Prince\n",
      "   Group: Book\n",
      "   Sales Rank: 249\n",
      "   Reviews: 212 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 205 | Out-Degree: 4\n",
      "\n",
      "15. Node 102158 (PageRank: 0.000867)\n",
      "   Title: Thelonious Monk with John Coltrane\n",
      "   Group: Music\n",
      "   Sales Rank: 613\n",
      "   Reviews: 27 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 73 | Out-Degree: 5\n",
      "\n",
      "16. Node 33247 (PageRank: 0.000864)\n",
      "   Title: Guess How Much I Love You (Guess How Much I Love You)\n",
      "   Group: Book\n",
      "   Sales Rank: 295\n",
      "   Reviews: 209 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 63 | Out-Degree: 5\n",
      "\n",
      "17. Node 477050 (PageRank: 0.000856)\n",
      "   Title: Saxophone Colossus\n",
      "   Group: Music\n",
      "   Sales Rank: 917\n",
      "   Reviews: 68 | Avg Rating: 5.0/5.0\n",
      "   In-Degree: 85 | Out-Degree: 5\n",
      "\n",
      "18. Node 98756 (PageRank: 0.000839)\n",
      "   Title: The Catcher in the Rye\n",
      "   Group: Book\n",
      "   Sales Rank: 60\n",
      "   Reviews: 2568 | Avg Rating: 4.0/5.0\n",
      "   In-Degree: 76 | Out-Degree: 5\n",
      "\n",
      "19. Node 546689 (PageRank: 0.000819)\n",
      "   Title: Pulp Fiction (Collector's Edition)\n",
      "   Group: DVD\n",
      "   Sales Rank: 109\n",
      "   Reviews: 633 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 93 | Out-Degree: 2\n",
      "\n",
      "20. Node 268597 (PageRank: 0.000795)\n",
      "   Title: Merriam-Webster's Spanish-English Dictionary\n",
      "   Group: Book\n",
      "   Sales Rank: 1102\n",
      "   Reviews: 7 | Avg Rating: 4.5/5.0\n",
      "   In-Degree: 35 | Out-Degree: 5\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "Product Categories in Top 20:\n",
      "  Book: 14\n",
      "  Music: 4\n",
      "  DVD: 2\n",
      "\n",
      "Average Rating: 4.62/5.0\n",
      "Average Reviews: 310.2\n",
      "Median Sales Rank: 250\n"
     ]
    }
   ],
   "source": [
    "# Load product information and look at the most popular products\n",
    "\n",
    "# Read the product metadata CSV\n",
    "products_df = pd.read_csv('metadata/products.csv')\n",
    "print(f\"Loaded {len(products_df)} products from metadata\\n\")\n",
    "\n",
    "# Set ProductId as the index so we can look up products by node ID\n",
    "products_df = products_df.set_index('ProductId')\n",
    "\n",
    "# Get the top 20 most popular products based on PageRank\n",
    "top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"Top 20 Most Popular Products (by PageRank)\")\n",
    "\n",
    "# Print info for each top product\n",
    "for rank, (node, pr_score) in enumerate(top_nodes, 1):\n",
    "    if node in products_df.index:\n",
    "        product = products_df.loc[node]\n",
    "        print(f\"\\n{rank}. Node {node} (PageRank: {pr_score:.6f})\")\n",
    "        print(f\"   Title: {product['title']}\")\n",
    "        print(f\"   Group: {product['group']}\")\n",
    "        print(f\"   Sales Rank: {int(product['salesrank']) if pd.notna(product['salesrank']) else 'N/A'}\")\n",
    "        print(f\"   Reviews: {int(product['total_review_count'])} | Avg Rating: {product['average_rating']:.1f}/5.0\")\n",
    "        print(f\"   In-Degree: {G.in_degree(node)} | Out-Degree: {G.out_degree(node)}\")\n",
    "    else:\n",
    "        print(f\"\\n{rank}. Node {node} (PageRank: {pr_score:.6f})\")\n",
    "        print(f\"   [Product metadata not available]\")\n",
    "        print(f\"   In-Degree: {G.in_degree(node)} | Out-Degree: {G.out_degree(node)}\")\n",
    "\n",
    "# Look at some summary stats for these top products\n",
    "\n",
    "print(\"\\nSummary Statistics\")\n",
    "\n",
    "top_node_ids = [node for node, _ in top_nodes if node in products_df.index]\n",
    "if top_node_ids:\n",
    "    top_products = products_df.loc[top_node_ids]\n",
    "    \n",
    "    print(\"\\nProduct Categories in Top 20:\")\n",
    "    group_counts = top_products['group'].value_counts()\n",
    "    for group, count in group_counts.items():\n",
    "        print(f\"  {group}: {count}\")\n",
    "    \n",
    "    print(f\"\\nAverage Rating: {top_products['average_rating'].mean():.2f}/5.0\")\n",
    "    print(f\"Average Reviews: {top_products['total_review_count'].mean():.1f}\")\n",
    "    print(f\"Median Sales Rank: {top_products['salesrank'].median():.0f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
